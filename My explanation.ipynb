{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdh1I3KJ-zcJ",
    "colab_type": "text"
   },
   "source": [
    "# Tutorial - Keras implementation of Deblur GAN with Demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeSJUjMuPtpu",
    "colab_type": "text"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "__Generative Adversarial Networks (GAN)__ is one of the most promising recent developments in Deep Learning. GAN, introduced by Ian Goodfellow in 2014, attacks the problem of unsupervised learning by training two deep networks, called _Generator_ and _Discriminator_, that compete and cooperate with each other. In the course of training, both networks eventually learn how to perform their tasks.\n",
    "\n",
    "Here is a brief of the research paper on DeblurGAN. The paper can be downloaded from [here](https://arxiv.org/pdf/1711.07064.pdf \"Deblur GAN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tocY0QprPxJy",
    "colab_type": "text"
   },
   "source": [
    "![alt text](https://cdn-images-1.medium.com/max/800/1*N4oqJsGmH-KZg3Vqrm_uYw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i7qgKvPQvMW",
    "colab_type": "text"
   },
   "source": [
    "To get a nice intuition consider it this way. \n",
    "\n",
    "Frank Abagale is a counterfiet artist. He knows how to make fake cheques. (Yup! That's from the movie - Catch me if You Can).\n",
    "\n",
    "Frank makes a fake cheque and show it to FBI. The FBI identifies it as a fake and tell Frank how they identified that. Our hero now improvises with the new little info and creates another fake. The FBI again identifies it's fakeness and tells Frank how they identified it. \n",
    "\n",
    "*This process of improving and providing feedback keeps going on untill Frank successfully fools the FBI by making a perfect counterfeit!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72j5H70QaIIA",
    "colab_type": "text"
   },
   "source": [
    "**Let's take each block one by one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_o9-awWCVvP",
    "colab_type": "text"
   },
   "source": [
    "\n",
    "\n",
    "## What is Blind Motion Deblurring and why we use GAN's for that?\n",
    " \n",
    "In simple words, removing the blurring effect caused due to motion of objects (or camera ) in images is called Motion Deblurring. The common formulation of non-uniform blur model is the following:\n",
    "\n",
    "\n",
    "  __IB = k(M) * IS + N__ where\n",
    "\n",
    "* __IB__ is a blurred image, \n",
    "\n",
    "* __k(M)__ are unknown blur kernels determined by motion field\n",
    "\n",
    "* operator __(*)__  denotes the convolution\n",
    "\n",
    "* __IS__ is the sharp latent\n",
    "\n",
    "* __N__ is an additive noise.\n",
    "\n",
    "\n",
    "If __k(M)__ is unknown the problem is classified as __Blind Deblurring__.\n",
    "\n",
    "We use GANs as they are known for the ability to preserve texture details in images, create solutions that are close to the real image manifold and look perceptually convincing.\n",
    "\n",
    "We are using GoPro dataset that consists of 2103 pairs of blurred and sharp images in 720p quality, taken from various scenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlUNWod7RMIc",
    "colab_type": "text"
   },
   "source": [
    "# The Generator Model\n",
    "\n",
    "The generator aims at reproducing sharp images. The network is based on ResNet blocks. It keeps track of the evolutions applied to the original blurred image\n",
    "\n",
    "The generator synthesizes fake images. In Figure 2, the fake image is generated from a 100-dimensional noise (uniform distribution between -1.0 to 1.0) using the inverse of convolution, called transposed convolution. Instead of fractionally-strided convolution as suggested in DCGAN, upsampling between the first three layers is used since it synthesizes more realistic handwriting images. In between layers, batch normalization stabilizes learning. The activation function after each layer is a ReLU. The output of the sigmoid at the last layer produces the fake image. Dropout of between 0.3 and 0.5 at the first layer prevents overfitting. Listing 2 shows the implementation in Keras.\n",
    "\n",
    "<img src=\"https://image.ibb.co/bMXkFT/image.png\" alt=\"image\" border=\"0\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Y7JzOnyaipI",
    "colab_type": "text"
   },
   "source": [
    "The core is 9 ResNet blocks applied to an upsampling of the original image. A ResNet block is defined as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K08JZ4pdadLm",
    "colab_type": "text"
   },
   "source": [
    "![alt text](https://cdn-images-1.medium.com/max/1000/1*OhuvC1YUdHyLbGO6rWWHhA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvgS5-6bEaR5",
    "colab_type": "text"
   },
   "source": [
    "First we take the input and pad it using the utility function ReflectionPadding2D defined in the layer_utils.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "LCWE2Kxm6TQ_",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "def res_block(input, filters, kernel_size=(3, 3), strides=(1, 1), use_dropout=False):\n",
    "    \"\"\"\n",
    "    Instanciate a Keras Resnet Block using sequential API.\n",
    "    :param input: Input tensor\n",
    "    :param filters: Number of filters to use\n",
    "    :param kernel_size: Shape of the kernel for the convolution\n",
    "    :param strides: Shape of the strides for the convolution\n",
    "    :param use_dropout: Boolean value to determine the use of dropout\n",
    "    :return: Keras Model\n",
    "    \"\"\"\n",
    "    x = ReflectionPadding2D((1, 1))(input)\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=strides,)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if use_dropout:\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "    x = ReflectionPadding2D((1, 1))(x)\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=kernel_size,\n",
    "               strides=strides,)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    merged = Add()([input, x])\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqBkveOfrBFD",
    "colab_type": "text"
   },
   "source": [
    "In D-GAN we create 9 ResNet blocks layer and pass the unsampled version of the input. To keep the output normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "rz8SqxOzq8Xa",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Activation, Add\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from layer_utils import ReflectionPadding2D, res_block\n",
    "\n",
    "ngf = 64\n",
    "input_nc = 3\n",
    "output_nc = 3\n",
    "input_shape_generator = (256, 256, input_nc)\n",
    "n_blocks_gen = 9\n",
    "\n",
    "\n",
    "def generator_model():\n",
    "    \"\"\"Build generator architecture.\"\"\"\n",
    "    # Current version : ResNet block\n",
    "    inputs = Input(shape=image_shape)\n",
    "\n",
    "    x = ReflectionPadding2D((3, 3))(inputs)\n",
    "    x = Conv2D(filters=ngf, kernel_size=(7,7), padding='valid')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Increase filter number\n",
    "    n_downsampling = 2\n",
    "    for i in range(n_downsampling):\n",
    "        mult = 2**i\n",
    "        x = Conv2D(filters=ngf*mult*2, kernel_size=(3,3), strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    # Apply 9 ResNet blocks\n",
    "    mult = 2**n_downsampling\n",
    "    for i in range(n_blocks_gen):\n",
    "        x = res_block(x, ngf*mult, use_dropout=True)\n",
    "\n",
    "    # Decrease filter number to 3 (RGB)\n",
    "    for i in range(n_downsampling):\n",
    "        mult = 2**(n_downsampling - i)\n",
    "        x = Conv2DTranspose(filters=int(ngf * mult / 2), kernel_size=(3,3), strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "\n",
    "    x = ReflectionPadding2D((3,3))(x)\n",
    "    x = Conv2D(filters=output_nc, kernel_size=(7,7), padding='valid')(x)\n",
    "    x = Activation('tanh')(x)\n",
    "\n",
    "    # Add direct connection from input to output and recenter to [-1, 1]\n",
    "    outputs = Add()([x, inputs])\n",
    "    outputs = Lambda(lambda z: z/2)(outputs)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='Generator')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXZzs_yPAI7W",
    "colab_type": "text"
   },
   "source": [
    "# The Discriminator Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCpTot875atC",
    "colab_type": "text"
   },
   "source": [
    "A discriminator that tells how real an image is, is basically a deep Convolutional Neural Network (CNN) as shown in Figure.\n",
    "\n",
    "The sigmoid output is a scalar value of the probability of how real the image is (0.0 is certainly fake, 1.0 is certainly real, anything in between is a gray area). \n",
    "\n",
    "The difference from a typical CNN is the absence of max-pooling in between layers. Instead, a strided convolution is used for downsampling.\n",
    "\n",
    "The activation function used in each CNN layer is a leaky ReLU. A dropout between 0.4 and 0.7 between layers prevent over fitting and memorization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AniCGe7w53iG",
    "colab_type": "text"
   },
   "source": [
    "<img src=\"https://image.ibb.co/gdJeaT/image.png\" alt=\"image\" border=\"0\">\n",
    "\n",
    "Here is the architecture for Discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "BJ5oMxUV5aTf",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "ndf = 64\n",
    "output_nc = 3\n",
    "input_shape_discriminator = (256, 256, output_nc)\n",
    "\n",
    "\n",
    "def discriminator_model():\n",
    "    \"\"\"Build discriminator architecture.\"\"\"\n",
    "    n_layers, use_sigmoid = 3, False\n",
    "    inputs = Input(shape=input_shape_discriminator)\n",
    "\n",
    "    x = Conv2D(filters=ndf, kernel_size=(4,4), strides=2, padding='same')(inputs)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf_mult, nf_mult_prev = 1, 1\n",
    "    for n in range(n_layers):\n",
    "        nf_mult_prev, nf_mult = nf_mult, min(2**n, 8)\n",
    "        x = Conv2D(filters=ndf*nf_mult, kernel_size=(4,4), strides=2, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    nf_mult_prev, nf_mult = nf_mult, min(2**n_layers, 8)\n",
    "    x = Conv2D(filters=ndf*nf_mult, kernel_size=(4,4), strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = Conv2D(filters=1, kernel_size=(4,4), strides=1, padding='same')(x)\n",
    "    if use_sigmoid:\n",
    "        x = Activation('sigmoid')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='tanh')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=x, name='Discriminator')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ2oQAl86-uk",
    "colab_type": "text"
   },
   "source": [
    "The last step is to build the full model. A particularity of this GAN is that inputs are real images and not noise. Therefore, we have a direct feedback on the generator’s outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "pvl4mKk56_vL",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "def generator_containing_discriminator_multiple_outputs(generator, discriminator):\n",
    "    inputs = Input(shape=image_shape)\n",
    "    generated_images = generator(inputs)\n",
    "    outputs = discriminator(generated_images)\n",
    "    model = Model(inputs=inputs, outputs=[generated_images, outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmOVwDQ27jfq",
    "colab_type": "text"
   },
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8dntM3a7nBp",
    "colab_type": "text"
   },
   "source": [
    "We extract losses at two levels, at the end of the generator and at the end of the full model.\n",
    "\n",
    "The first one is a perceptual loss computed directly on the generator’s outputs. This first loss ensures the GAN model is oriented towards a deblurring task. It compares the outputs of the first convolutions of VGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "08crlzr78UWh",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "\n",
    "image_shape = (256, 256, 3)\n",
    "\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    vgg = VGG16(include_top=False, weights='imagenet', input_shape=image_shape)\n",
    "    loss_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)\n",
    "    loss_model.trainable = False\n",
    "    return K.mean(K.square(loss_model(y_true) - loss_model(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxCY_aBg8WqM",
    "colab_type": "text"
   },
   "source": [
    "The second loss is the Wasserstein loss performed on the outputs of the whole model. It takes the mean of the differences between two images. It is known to improve convergence of generative adversarial networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "B6yTpAzg8XpM",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true*y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRKFItIk8bby",
    "colab_type": "text"
   },
   "source": [
    "## Training the Model\n",
    "The first step is to load the data and initialize all the models. We use our custom function to load the dataset, and add Adam optimizers for our models. We set the Keras trainable option to prevent the discriminator from training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "IKJk8lvh8gEu",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = load_images('./images/train', n_images)\n",
    "y_train, x_train = data['B'], data['A']\n",
    "\n",
    "# Initialize models\n",
    "g = generator_model()\n",
    "d = discriminator_model()\n",
    "d_on_g = generator_containing_discriminator_multiple_outputs(g, d)\n",
    "\n",
    "# Initialize optimizers\n",
    "g_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "d_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "d_on_g_opt = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "# Compile models\n",
    "d.trainable = True\n",
    "d.compile(optimizer=d_opt, loss=wasserstein_loss)\n",
    "d.trainable = False\n",
    "loss = [perceptual_loss, wasserstein_loss]\n",
    "loss_weights = [100, 1]\n",
    "d_on_g.compile(optimizer=d_on_g_opt, loss=loss, loss_weights=loss_weights)\n",
    "d.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8uVSV_M8kYK",
    "colab_type": "text"
   },
   "source": [
    "Then, we start launching the epochs and divide the dataset into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "FtM5vLR-8lDV",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(epoch_num):\n",
    "  print('epoch: {}/{}'.format(epoch, epoch_num))\n",
    "  print('batches: {}'.format(x_train.shape[0] / batch_size))\n",
    "\n",
    "  # Randomize images into batches\n",
    "  permutated_indexes = np.random.permutation(x_train.shape[0])\n",
    "\n",
    "  for index in range(int(x_train.shape[0] / batch_size)):\n",
    "      batch_indexes = permutated_indexes[index*batch_size:(index+1)*batch_size]\n",
    "      image_blur_batch = x_train[batch_indexes]\n",
    "      image_full_batch = y_train[batch_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePRsmv-H8qMb",
    "colab_type": "text"
   },
   "source": [
    "Finally, we successively train the discriminator and the generator, based on both losses. We generate fake inputs with the generator. We train the discriminator to distinguish fake from real inputs, and we train the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "bUGrMlgO8tgT",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     }
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(epoch_num):\n",
    "  for index in range(batches):\n",
    "    # [Batch Preparation]\n",
    "\n",
    "    # Generate fake inputs\n",
    "    generated_images = g.predict(x=image_blur_batch, batch_size=batch_size)\n",
    "    \n",
    "    # Train multiple times discriminator on real and fake inputs\n",
    "    for _ in range(critic_updates):\n",
    "        d_loss_real = d.train_on_batch(image_full_batch, output_true_batch)\n",
    "        d_loss_fake = d.train_on_batch(generated_images, output_false_batch)\n",
    "        d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "    d.trainable = False\n",
    "    # Train generator only on discriminator's decision and generated images\n",
    "    d_on_g_loss = d_on_g.train_on_batch(image_blur_batch, [image_full_batch, output_true_batch])\n",
    "\n",
    "    d.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu9Tj1o089HN",
    "colab_type": "text"
   },
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oet0yU-9AoXd",
    "colab_type": "text"
   },
   "source": [
    "Here is the execution of deblurring an artificially blurred (street.png). \n",
    "### Note we are using pretrained models as fresh execution of the code explained above requires abundant execution time and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vGw0GcQq9AS5",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 90.0
    },
    "outputId": "a5eb09dd-3c24-4eeb-d9cb-d88d4c8691b5",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.530374720859E12,
     "user_tz": -330.0,
     "elapsed": 6261.0,
     "user": {
      "displayName": "Kshitij Mathur",
      "photoUrl": "//lh3.googleusercontent.com/-j5MvCtbBmtQ/AAAAAAAAAAI/AAAAAAAAXug/f07pZ3A5czw/s50-c-k-no/photo.jpg",
      "userId": "114525469442523041092"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'deblur-gan-tutorial'...\n",
      "remote: Counting objects: 72, done.\u001b[K\n",
      "remote: Total 72 (delta 0), reused 0 (delta 0), pack-reused 72\u001b[K\n",
      "Unpacking objects: 100% (72/72), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mathurk29/deblur-gan-tutorial.git #A file by name deblura.png should get downloaded after running  this and following code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PUutDUXU-JqJ",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1383.0
    },
    "outputId": "a0fc4e90-17cf-4c99-a925-0f8353464237",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.530374804615E12,
     "user_tz": -330.0,
     "elapsed": 16066.0,
     "user": {
      "displayName": "Kshitij Mathur",
      "photoUrl": "//lh3.googleusercontent.com/-j5MvCtbBmtQ/AAAAAAAAAAI/AAAAAAAAXug/f07pZ3A5czw/s50-c-k-no/photo.jpg",
      "userId": "114525469442523041092"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==0.1.9 (from -r deblur-gan-tutorial/requirements.txt (line 1))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/3c/1985d86a44bfe44fd060c02807336f840a509bfaa2d340860fba7d22da39/absl-py-0.1.9.tar.gz (79kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 4.3MB/s \n",
      "\u001b[?25hCollecting bleach==1.5.0 (from -r deblur-gan-tutorial/requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting click==6.7 (from -r deblur-gan-tutorial/requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl (71kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 7.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r deblur-gan-tutorial/requirements.txt (line 4)) (0.10.0)\n",
      "Collecting decorator==4.2.1 (from -r deblur-gan-tutorial/requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/5a/53db15bf367d2028bdc6700dbdf1bdfab46b9f208b7516952817c0808118/decorator-4.2.1-py2.py3-none-any.whl\n",
      "Collecting futures==3.1.1 (from -r deblur-gan-tutorial/requirements.txt (line 6))\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/26/b61e3a4eb50653e8a7339d84eeaa46d1e93b92951978873c220ae64d0733/futures-3.1.1.tar.gz\n",
      "Collecting h5py==2.7.1 (from -r deblur-gan-tutorial/requirements.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/b8/a63fcc840bba5c76e453dd712dbca63178a264c8990e0086b72965d4e954/h5py-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.4MB 8.7MB/s \n",
      "\u001b[?25hCollecting html5lib==0.9999999 (from -r deblur-gan-tutorial/requirements.txt (line 8))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 26.0MB/s \n",
      "\u001b[?25hCollecting imageio==2.2.0 (from -r deblur-gan-tutorial/requirements.txt (line 9))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/4b/f68b2a2e3e98bc006f54ff6fee50f20c7ca4052ec6da68c58c299c7bedf5/imageio-2.2.0.tar.gz (3.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.3MB 12.5MB/s \n",
      "\u001b[?25hCollecting Keras==2.1.3 (from -r deblur-gan-tutorial/requirements.txt (line 10))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/ae/7f94a03cb3f74cdc8a0f5f86d1df5c1dd686acb9a9c2a421c64f8497358e/Keras-2.1.3-py2.py3-none-any.whl (319kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 24.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: Markdown==2.6.11 in /usr/local/lib/python3.6/dist-packages (from -r deblur-gan-tutorial/requirements.txt (line 11)) (2.6.11)\n",
      "Requirement already satisfied: matplotlib==2.1.2 in /usr/local/lib/python3.6/dist-packages (from -r deblur-gan-tutorial/requirements.txt (line 12)) (2.1.2)\n",
      "Requirement already satisfied: networkx==2.1 in /usr/local/lib/python3.6/dist-packages (from -r deblur-gan-tutorial/requirements.txt (line 13)) (2.1)\n",
      "Collecting numpy==1.14.0 (from -r deblur-gan-tutorial/requirements.txt (line 14))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/ac/5c270dffb864f23315e9c1f9e0a0b300c797b3c170666c031c4de42aacae/numpy-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (17.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.2MB 2.1MB/s \n",
      "\u001b[?25hCollecting Pillow==5.0.0 (from -r deblur-gan-tutorial/requirements.txt (line 15))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/2f/86941111d108fd060190c994f15881283b98693c1c370e74885cfc470eb3/Pillow-5.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.9MB 8.8MB/s \n",
      "\u001b[?25hCollecting pip-autoremove==0.9.0 (from -r deblur-gan-tutorial/requirements.txt (line 16))\n",
      "  Downloading https://files.pythonhosted.org/packages/ba/39/dec355f61fabe35689acd1879ce17492863930095df5992f8276eab1b0b8/pip_autoremove-0.9.0-py2.py3-none-any.whl\n",
      "Collecting pkg-resources==0.0.0 (from -r deblur-gan-tutorial/requirements.txt (line 17))\n",
      "\u001b[31m  Could not find a version that satisfies the requirement pkg-resources==0.0.0 (from -r deblur-gan-tutorial/requirements.txt (line 17)) (from versions: )\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mNo matching distribution found for pkg-resources==0.0.0 (from -r deblur-gan-tutorial/requirements.txt (line 17))\u001b[0m\n",
      "Collecting absl-py==0.1.9 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 1))\n",
      "  Using cached https://files.pythonhosted.org/packages/42/3c/1985d86a44bfe44fd060c02807336f840a509bfaa2d340860fba7d22da39/absl-py-0.1.9.tar.gz\n",
      "Collecting bleach==1.5.0 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 2))\n",
      "  Using cached https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
      "Collecting click==6.7 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r deblur-gan-tutorial/requirements-gpu.txt (line 4)) (0.10.0)\n",
      "Collecting decorator==4.2.1 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 5))\n",
      "  Using cached https://files.pythonhosted.org/packages/e1/5a/53db15bf367d2028bdc6700dbdf1bdfab46b9f208b7516952817c0808118/decorator-4.2.1-py2.py3-none-any.whl\n",
      "Collecting futures==3.1.1 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 6))\n",
      "  Using cached https://files.pythonhosted.org/packages/cc/26/b61e3a4eb50653e8a7339d84eeaa46d1e93b92951978873c220ae64d0733/futures-3.1.1.tar.gz\n",
      "Collecting h5py==2.7.1 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 7))\n",
      "  Using cached https://files.pythonhosted.org/packages/f2/b8/a63fcc840bba5c76e453dd712dbca63178a264c8990e0086b72965d4e954/h5py-2.7.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting html5lib==0.9999999 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 8))\n",
      "  Using cached https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz\n",
      "Collecting imageio==2.2.0 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 9))\n",
      "  Using cached https://files.pythonhosted.org/packages/a0/4b/f68b2a2e3e98bc006f54ff6fee50f20c7ca4052ec6da68c58c299c7bedf5/imageio-2.2.0.tar.gz\n",
      "Collecting Keras==2.1.3 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 10))\n",
      "  Using cached https://files.pythonhosted.org/packages/08/ae/7f94a03cb3f74cdc8a0f5f86d1df5c1dd686acb9a9c2a421c64f8497358e/Keras-2.1.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Markdown==2.6.11 in /usr/local/lib/python3.6/dist-packages (from -r deblur-gan-tutorial/requirements-gpu.txt (line 11)) (2.6.11)\n",
      "Requirement already satisfied: matplotlib==2.1.2 in /usr/local/lib/python3.6/dist-packages (from -r deblur-gan-tutorial/requirements-gpu.txt (line 12)) (2.1.2)\n",
      "Requirement already satisfied: networkx==2.1 in /usr/local/lib/python3.6/dist-packages (from -r deblur-gan-tutorial/requirements-gpu.txt (line 13)) (2.1)\n",
      "Collecting numpy==1.14.0 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 14))\n",
      "  Using cached https://files.pythonhosted.org/packages/dc/ac/5c270dffb864f23315e9c1f9e0a0b300c797b3c170666c031c4de42aacae/numpy-1.14.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting Pillow==5.0.0 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 15))\n",
      "  Using cached https://files.pythonhosted.org/packages/9a/2f/86941111d108fd060190c994f15881283b98693c1c370e74885cfc470eb3/Pillow-5.0.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting pip-autoremove==0.9.0 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 16))\n",
      "  Using cached https://files.pythonhosted.org/packages/ba/39/dec355f61fabe35689acd1879ce17492863930095df5992f8276eab1b0b8/pip_autoremove-0.9.0-py2.py3-none-any.whl\n",
      "Collecting pkg-resources==0.0.0 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 17))\n",
      "\u001b[31m  Could not find a version that satisfies the requirement pkg-resources==0.0.0 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 17)) (from versions: )\u001b[0m\n",
      "\u001b[31mNo matching distribution found for pkg-resources==0.0.0 (from -r deblur-gan-tutorial/requirements-gpu.txt (line 17))\u001b[0m\n",
      "Collecting click\n",
      "  Using cached https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl\n",
      "Installing collected packages: click\n",
      "Successfully installed click-6.7\n"
     ]
    }
   ],
   "source": [
    "!pip install -r deblur-gan-tutorial/requirements.txt\n",
    "!pip install -r deblur-gan-tutorial/requirements-gpu.txt\n",
    "!pip install click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C1x0iDgv-YIf",
    "colab_type": "code",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0.0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35.0
    },
    "outputId": "fda73ba8-da9a-4b3b-93d0-2528d207946d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.530374870553E12,
     "user_tz": -330.0,
     "elapsed": 14560.0,
     "user": {
      "displayName": "Kshitij Mathur",
      "photoUrl": "//lh3.googleusercontent.com/-j5MvCtbBmtQ/AAAAAAAAAAI/AAAAAAAAXug/f07pZ3A5czw/s50-c-k-no/photo.jpg",
      "userId": "114525469442523041092"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/content/deblur-gan-tutorial\")\n",
    "!python deblur_image.py --image_path a.png\n",
    "from google.colab import files\n",
    "files.download('deblura.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Kshitij_Mathur_Final_Submission.ipynb",
   "version": "0.3.2",
   "views": {},
   "default_view": {},
   "provenance": [
    {
     "file_id": "1P4nilgfa-DyW5oNlYn1GbbK9qLifWcUk",
     "timestamp": 1.529848346909E12
    }
   ],
   "collapsed_sections": [
    "yeSJUjMuPtpu"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}